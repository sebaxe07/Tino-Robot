\section{Social Robotics Foundations}
This section will establish the theoretical and practical foundations of social robotics that underpin the Tino V2 project development. The evolution of social robotics will be traced, covering the progression from industrial automation to human-centered robotic systems designed for interaction, companionship, and assistance in social environments, the emergence of telepresence robotics as a solution for remote communication and presence, and the integration of artificial intelligence and machine learning technologies to enable more natural and responsive human-robot interactions. The key characteristics of effective social robots will be examined, including the importance of predictable and reliable behavior that builds user trust and confidence, the need for intuitive interaction modalities that accommodate users with varying technical expertise, and the critical role of robust localization and navigation systems that enable autonomous operation in human environments. The current research landscape will be surveyed, covering major social robotics platforms and their capabilities, ongoing research in human-robot interaction psychology and user experience design, and the technological challenges that continue to limit widespread adoption of social robotics systems.

\section{Simultaneous Localization and Mapping (SLAM)}
This section will provide comprehensive coverage of SLAM technologies and their application to mobile robotics, with particular focus on the challenges and solutions relevant to indoor social robot operation. The SLAM problem formulation will be established, covering the fundamental challenge of simultaneously building a map of an unknown environment while determining the robot's location within that environment, the mathematical foundations including probabilistic approaches and uncertainty representation, and the trade-offs between computational complexity and accuracy that affect real-time implementation. Visual SLAM approaches will be examined in detail, covering feature-based methods like ORB-SLAM that rely on distinctive visual landmarks for tracking and mapping, direct methods that use raw pixel intensities for dense reconstruction, and hybrid approaches that combine the advantages of both feature-based and direct techniques. The specific capabilities and limitations of RTABMap will be analyzed, including its strength in loop closure detection that enables long-term mapping without drift accumulation, the RGB-D processing pipeline that leverages depth information for robust feature matching, and the memory management strategies that allow operation in large environments without excessive computational overhead. Common challenges in indoor SLAM will be addressed, covering the impact of dynamic environments with moving objects and people, the difficulties posed by textureless surfaces and repetitive patterns that provide insufficient visual features, and the accumulation of odometry drift that can lead to mapping inconsistencies over extended operation periods.

\section{Ultra-Wideband (UWB) Positioning Technology}
This section will examine UWB positioning technology as a complementary localization solution that addresses the limitations of visual SLAM in indoor environments. The UWB technology fundamentals will be established, covering the physics of ultra-wideband radio signals and their propagation characteristics that enable precise time-of-flight measurements, the anchor-based positioning architecture that provides absolute coordinate references, and the centimeter-level accuracy achievable under ideal conditions. The advantages of UWB for indoor robotics will be detailed, including immunity to visual challenges like poor lighting or featureless environments that can disrupt camera-based systems, the ability to provide absolute positioning without drift accumulation, and the low power consumption that makes UWB suitable for battery-powered mobile robots. The integration challenges will be addressed, covering the calibration requirements for anchor placement and coordinate system alignment, the impact of non-line-of-sight conditions and multipath effects in indoor environments, and the need for sensor fusion techniques to combine UWB position data with orientation information from other sources. Practical implementation considerations will be examined, including the setup and maintenance requirements for UWB anchor infrastructure, the trade-offs between positioning accuracy and update rates, and the strategies for handling temporary signal loss or degraded accuracy conditions.

\section{Computer Vision and Human Detection}
This section will cover the computer vision technologies employed for human detection and pose estimation in the Tino V2 system. The evolution of object detection algorithms will be traced, covering the progression from traditional computer vision techniques to modern deep learning approaches, the development of real-time detection frameworks like YOLO (You Only Look Once) that enable practical implementation on embedded hardware, and the specific advances in YOLOv11 that provide improved accuracy and efficiency for human detection tasks. Human pose estimation fundamentals will be established, covering the mathematical representation of human skeletal structure using key body joints, the challenges of 2D to 3D pose reconstruction from single camera views, and the integration of depth information to provide accurate 3D human tracking capabilities. The TensorRT optimization framework will be examined, covering the model conversion and optimization processes that enable real-time inference on NVIDIA hardware, the trade-offs between model complexity and inference speed that affect practical deployment, and the memory management strategies required for efficient operation on embedded systems with limited resources. Practical challenges in human detection will be addressed, covering the impact of varying lighting conditions and camera perspectives on detection accuracy, the handling of multiple people in the scene with potential occlusions, and the integration of detection results with robot control systems for responsive human-robot interaction.

\section{Virtual Reality Integration and Telepresence}
This section will examine the technologies and methodologies for integrating VR systems with physical robotics platforms to enable immersive telepresence experiences. The VR development landscape will be surveyed, covering the Unity game engine as a platform for creating interactive VR applications, the hardware requirements and capabilities of modern VR headsets for spatial tracking and user interaction, and the software frameworks that enable communication between VR applications and external systems. The telepresence robotics concept will be established, covering the psychological and technical requirements for effective remote presence that makes users feel genuinely connected to distant environments, the importance of low-latency communication for natural interaction, and the challenges of mapping human movements and intentions to appropriate robot behaviors. The communication architecture will be detailed, covering the ROS-TCP-Endpoint framework that enables Unity-ROS2 integration, the message passing protocols required for real-time data exchange, and the synchronization challenges that arise when coordinating virtual and physical systems with different update rates and latency characteristics. The user experience design considerations will be addressed, covering the development of intuitive VR interfaces that enable effective robot control without requiring technical expertise, the importance of providing appropriate feedback to help users understand robot state and capabilities, and the safety considerations that must be incorporated when enabling remote control of physical robotic systems.
