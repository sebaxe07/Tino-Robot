This chapter establishes the theoretical and technological foundations necessary for understanding the Tino V2 system development. The evolution from the original Tino robot to Tino V2 required comprehensive analysis of existing technologies across multiple domains: simultaneous localization and mapping (SLAM), human detection and pose estimation, social robotics principles, and virtual reality integration techniques. Each technology area presents distinct challenges and opportunities that directly influence the design decisions made in Tino V2. The chapter begins with an examination of SLAM technologies, which form the cornerstone of autonomous robot navigation and environmental awareness. Following this, human detection and pose estimation technologies are analyzed for their role in enabling natural human-robot interaction. The principles of social robotics and human-robot interaction provide the theoretical framework for understanding how robots can effectively engage with humans in social contexts. Virtual reality integration techniques are then explored as emerging tools for enhancing robot control and telepresence capabilities. Finally, a detailed analysis of the legacy Tino system identifies specific limitations that motivated the comprehensive redesign undertaken in this work. This background analysis directly informs the technical requirements and design choices presented in subsequent chapters, providing the necessary context for understanding how Tino V2 addresses the limitations of existing approaches while advancing the state-of-the-art in social robotics.

\section{Simultaneous Localization and Mapping (SLAM) Technologies}
Simultaneous Localization and Mapping (SLAM) represents one of the fundamental challenges in autonomous mobile robotics, particularly for social robots operating in dynamic indoor environments~\cite{durrant2006simultaneous}. SLAM systems enable robots to navigate in unknown environments while building environmental maps, providing the dual advantage of localization and environmental perception~\cite{scaramuzza2011visual}. This section provides a comprehensive analysis of available SLAM and localization technologies, examining their theoretical foundations, practical implementations, and suitability for social robotics applications.

\subsection{Image-Based SLAM Methods}
Image-based SLAM systems utilize visual information from cameras to perform simultaneous localization and mapping. These approaches can be categorized into feature-based, direct, and semi-direct methods, each with distinct advantages and limitations for social robotics applications.

\subsubsection{Feature-Based SLAM Systems}
Monocular visual SLAM systems, exemplified by PTAM~\cite{klein2007parallel} and later ORB-SLAM~\cite{mur2015orb}, utilize feature extraction and matching to estimate camera motion and reconstruct environmental structure. While computationally efficient, these systems suffer from inherent scale ambiguity and require careful initialization procedures to establish metric scale.

ORB-SLAM3~\cite{campos2021orbslam3} represents the current state-of-the-art in feature-based SLAM systems, supporting multiple sensor configurations including monocular, stereo, and RGB-D cameras. The system demonstrates robust performance in feature-rich environments through sophisticated ORB feature matching algorithms and advanced loop closure detection mechanisms. The system offers several advantages including proven accuracy in academic benchmarks such as TUM RGB-D and EuRoC datasets, support for multiple sensor modalities, and robust handling of dynamic environments.

However, practical implementation reveals significant challenges including substantial computational requirements that typically necessitate GPU optimization for real-time performance~\cite{huang2021sensor}. The system can struggle in textureless environments or under poor lighting conditions where ORB feature extraction becomes unreliable. Compilation and integration challenges, particularly on ARM64 architectures, can also present development obstacles.

Stereo visual odometry addresses the scale ambiguity problem by utilizing depth information from calibrated stereo camera pairs. These approaches provide accurate trajectory estimation through triangulation-based depth reconstruction, though they face significant challenges in textureless environments or under poor lighting conditions where feature matching becomes unreliable~\cite{geiger2011stereoscan}.

\subsubsection{Direct and Semi-Direct SLAM Systems}
Semi-direct Visual Odometry (SVO)~\cite{forster2014svo} offers reduced computational requirements compared to feature-based SLAM systems by tracking pixels directly rather than extracting and matching discrete features. The system demonstrates particular compatibility with fisheye and catadioptric cameras, making it suitable for wide field-of-view applications.

Direct Sparse Odometry (DSO)~\cite{engel2017direct} represents another approach in this category, utilizing direct photometric error minimization without feature extraction. While these systems offer computational advantages, they require well-textured environments for reliable tracking and can struggle in highly dynamic scenes. SVO presents compilation challenges on ARM64 architectures and lacks comprehensive map management capabilities.

\subsubsection{Graph-Based SLAM Systems}
RTABMap (Real-Time Appearance-Based Mapping)~\cite{labbe2019rtab} employs appearance-based mapping techniques combined with graph optimization to provide robust performance across various environmental conditions. The system offers comprehensive map management capabilities including reliable save and load functionality, multi-session mapping support, and effective relocalization performance.

RTABMap demonstrates particular strength in practical deployment scenarios, offering stable map persistence and reliable relocalization capabilities essential for operational robot systems. The system provides excellent integration with standard robotics frameworks including ROS and ROS2, supporting various sensor configurations including RGB-D cameras and stereo systems.

The integration of RGB-D sensors enables more reliable feature tracking and geometric reconstruction compared to traditional stereo approaches. However, these systems face limitations including reduced operating range (typically under 5 meters) and sensitivity to lighting conditions that affect depth sensor performance~\cite{henry2012rgb}. The primary advantages include robust map management, reliable relocalization, excellent framework integration, and proven performance in real-world deployments, making RTABMap particularly suitable for applications requiring consistent long-term operation and map reuse capabilities.

\subsection{Ultra-Wideband Positioning Technology}
Ultra-Wideband (UWB) technology has gained significant attention for indoor positioning applications due to its potential for centimeter-level accuracy~\cite{gezici2005localization}. The technology operates in the 3.1--10.6 GHz frequency range, providing high temporal resolution that enables precise time-of-flight distance measurements.

UWB positioning systems require infrastructure deployment including multiple anchor points with known positions to enable triangulation-based localization. Research has demonstrated the technology's advantages including low power consumption, minimal interference with other wireless systems, and potential penetration through materials including fabrics~\cite{rpo2019uwb}.

However, UWB systems face significant challenges in Non-Line-of-Sight (NLOS) conditions where multipath effects can degrade positioning accuracy~\cite{luo2020uwb}. The technology primarily provides position information and requires additional sensors for orientation estimation, typically through fusion with inertial measurement units.

\subsection{LiDAR-Based SLAM}
LiDAR systems provide high-resolution 3D environmental mapping through laser scanning, offering excellent spatial resolution and range performance~\cite{yan2019lidar}. These systems enable accurate simultaneous localization and mapping through point cloud analysis and clustering algorithms, providing robust environmental perception capabilities.

LiDAR-based SLAM offers several advantages including operation in various lighting conditions, precise distance measurements, and detailed environmental reconstruction. The technology is particularly effective for large-scale outdoor environments and can provide reliable localization even in feature-poor environments where visual SLAM systems might struggle.

However, LiDAR systems present several challenges for social robotics applications. The mechanical scanning components can be sensitive to vibration, making them unsuitable for robots with soft or flexible structures. The cost and size of LiDAR systems can also be prohibitive for many social robot applications. Additionally, the range and resolution capabilities of LiDAR systems may represent overkill for typical indoor social interaction scenarios.

\subsection{Sensor Fusion Approaches}
Modern localization systems increasingly rely on sensor combination techniques to integrate information from multiple sensing modalities. Extended Kalman Filter (EKF) and particle filter approaches enable fusion of visual odometry, IMU data, and wheel encoder information to achieve robust localization performance~\cite{huang2021sensor}. However, implementing full probabilistic fusion requires significant development effort and computational resources.

Inertial Measurement Units (IMUs) combined with wheel encoders represent a low-cost localization approach suitable for many mobile robot applications. While these systems offer advantages including low cost and independence from environmental conditions, they suffer from drift accumulation over time due to integration of noisy sensor measurements~\cite{borenstein1996measurement}. Another fundamental limitation is their open-loop nature: these systems cannot verify whether their position estimates are correct or implement corrective measures, unlike vision-based or LiDAR-based SLAM systems that can match environmental features to detect and correct localization errors.

Performance can be particularly poor on uneven surfaces where wheel slip affects encoder accuracy, or in applications involving impulse-based movement patterns that can saturate IMU sensors. The drift characteristics make these approaches unsuitable as standalone solutions for long-term autonomous operation.

\section{Human Detection and Pose Estimation Technologies}
Human detection and tracking capabilities are essential for social robots that must operate safely and effectively in human-populated environments. This section examines various sensing modalities and computational approaches for human perception in robotics applications.

\subsection{Computer Vision-Based Human Detection}
\subsubsection{RGB-D Sensing Systems}
RGB-D cameras provide simultaneous color and depth information, enabling robust human detection and pose estimation. Intel RealSense cameras and similar structured light devices offer synchronized data streams that support skeleton tracking through established frameworks.

OpenPose~\cite{cao2019openpose} represents a breakthrough in real-time multi-person pose estimation, utilizing Part Affinity Fields (PAFs) to associate body parts with individuals in crowded scenes. The system provides detailed information about human posture and gesture, enabling rich interaction capabilities for social robotics applications.

MediaPipe~\cite{lugaresi2019mediapipe} offers an alternative framework optimized for mobile and embedded platforms, providing real-time pose estimation with reduced computational requirements. These systems demonstrate comprehensive human pose information extraction with reasonable computational demands.

Integration challenges arise when cameras must be concealed within robot structures, particularly for robots with soft exteriors where camera visibility may be constrained.

\subsubsection{Machine Learning-Enhanced Detection}
Modern deep learning approaches have revolutionized human detection capabilities. YOLO (You Only Look Once) architectures, particularly YOLOv8~\cite{redmon2016you} and recent variants including YOLOv11, provide real-time human detection and pose estimation capabilities with single-stage detection frameworks.

These systems offer several advantages including lower hardware costs, reduced physical profile compared to RGB-D cameras, and sophisticated pose estimation capabilities. Monocular depth estimation networks such as MiDaS~\cite{ranftl2020towards} can provide approximate depth information without requiring specialized depth sensors.

The primary challenges include computational requirements for real-time processing on embedded platforms and dependency on lighting conditions. However, advances in embedded AI processing platforms, including NVIDIA Jetson series and specialized AI accelerators, are making these approaches increasingly viable for mobile robotics applications.

\subsection{Alternative Sensing Modalities}
\subsubsection{Thermal Imaging}
Thermal cameras detect infrared radiation emitted by objects, making them particularly effective for human detection regardless of lighting conditions~\cite{thermal2019detection}. Thermal imaging offers potential advantages including operation in complete darkness and possible penetration through certain materials including fabrics.

The primary benefit for social robotics applications is the ability to detect human presence even when optical cameras may be obscured. However, thermal sensors provide limited contextual information beyond heat signatures, making it difficult to extract detailed pose information or distinguish between different individuals.

\subsubsection{LiDAR-Based Detection}
LiDAR systems provide high-resolution 3D environmental mapping through laser scanning, offering excellent spatial resolution and range performance~\cite{yan2019lidar}. These systems can accurately detect human presence and track movement through point cloud analysis and clustering algorithms.

However, LiDAR systems present several challenges for social robotics applications. The mechanical scanning components can be sensitive to vibration, making them unsuitable for robots with soft or flexible structures. The cost and size of LiDAR systems can also be prohibitive for many social robot applications.

\section{Social Robotics and Human-Robot Interaction Foundations}
Social robotics has emerged as a distinct field focusing on robots designed to interact with humans in natural, socially meaningful ways~\cite{breazeal2003toward}. Understanding the theoretical foundations of human-robot interaction provides essential context for the technological requirements of social robots.

\subsection{Non-Verbal Communication in Robotics}
Research in human communication demonstrates that non-verbal cues carry significant emotional weight, often conveying information that verbal communication cannot express~\cite{mehrabian1971silent}. Ekman's seminal work on universal facial expressions shows that fundamental emotions are communicated across cultures through body language and facial expressions~\cite{ekman1971universals}.

These insights have profound implications for social robotics design. Robots capable of expressing emotions through coordinated physical movements can potentially achieve meaningful communication without relying on anthropomorphic features or verbal interaction~\cite{breazeal2003toward}. This approach enables exploration of movement as a primary communicative tool, independent of human anatomical associations.

\subsection{Telepresence and Mediated Interaction}
Telepresence robotics research investigates how humans can effectively control remote robotic systems to achieve natural interaction with distant environments~\cite{sheridan1992musings}. The field has evolved from simple teleoperation to sophisticated systems that preserve social presence and emotional connection across physical distances.

Recent advances in virtual reality technology have opened new possibilities for immersive robot control, where operators can experience robot embodiment through first-person perspectives. This approach enables investigation of how virtual reality interfaces can enhance rather than diminish the emotional and empathetic qualities of robot-mediated human interaction.

\section{VR Integration in Robotics}
Virtual Reality integration with robotics systems represents an emerging area of research with significant potential for advancing human-robot interaction capabilities. This section examines the current state of VR-robotics integration and its implications for social robotics applications.

\subsection{Immersive Teleoperation Systems}
Traditional robot teleoperation relies on external displays and control interfaces that create cognitive distance between the operator and the robot~\cite{sheridan1992musings}. Immersive VR interfaces offer the potential to bridge this gap by providing first-person robot perspectives and natural control paradigms.

Research in VR teleoperation has demonstrated improved operator performance and reduced cognitive load when controlling robots through immersive interfaces. These systems enable operators to leverage natural spatial reasoning and motor skills for robot control, potentially improving the quality and naturalness of robot movements.

\subsection{Challenges in VR-Robot Integration}
Successful VR-robot integration faces several technical challenges including latency requirements, sensor data processing, and spatial correspondence between virtual and physical environments. Real-time performance requirements typically mandate processing latencies below 100 milliseconds to maintain immersive experiences and prevent motion sickness in VR applications.

The integration requires sophisticated data processing pipelines to convert robot sensor data into meaningful VR representations while maintaining real-time performance constraints. Spatial registration between virtual environments and physical robot spaces presents additional challenges, particularly when virtual environments are custom-designed rather than directly mapped from physical spaces.



