\section{Technology Research and Selection Methodology}
This section will detail the systematic approach undertaken to research and select appropriate technologies for the Tino V2 system redesign. The research methodology will be established, covering the comprehensive literature review process used to identify state-of-the-art solutions in SLAM, sensor fusion, human detection, and VR integration, the evaluation criteria developed to assess technologies based on accuracy, computational requirements, integration complexity, and long-term maintenance considerations, and the experimental validation approach used to test promising technologies before final selection. The localization technology analysis will be presented, covering the systematic comparison of visual odometry approaches including ORB-SLAM3, SVO, and RTABMap with detailed evaluation of their computational requirements and accuracy performance, the assessment of UWB positioning systems including anchor configuration requirements and accuracy characteristics under various environmental conditions, and the analysis of sensor fusion approaches that combine complementary positioning technologies to address individual limitations. The human detection technology evaluation will be detailed, covering the comparison of different YOLO variants and their performance on embedded hardware, the assessment of depth camera technologies including Intel RealSense and Oak-D Pro systems, and the analysis of optimization frameworks like TensorRT for real-time inference on NVIDIA platforms. The VR integration technology selection will be examined, covering the evaluation of Unity versus other VR development platforms, the assessment of communication frameworks for ROS2-VR integration, and the analysis of VR hardware compatibility and user experience considerations.

\section{System Architecture Design}
This section will present the comprehensive system architecture developed for the Tino V2 platform, covering both hardware and software components and their integration strategies. The overall architecture philosophy will be established, covering the modular design approach that enables independent development and testing of subsystems, the ROS2-based communication framework that provides standardized interfaces between components, and the scalability considerations that allow for future expansion and modification of system capabilities. The hardware architecture will be detailed, covering the migration from Raspberry Pi to NVIDIA Orin Nano as the primary computing platform with analysis of performance improvements and power consumption considerations, the integration of Oak-D Pro depth camera for simultaneous SLAM and human detection capabilities, the UWB positioning system implementation including anchor placement strategies and coordinate system alignment, and the redesigned mechanical systems including the differential drive base and improved Stewart platform head mechanism. The software architecture will be presented, covering the ROS2 node structure that organizes functionality into discrete, communicating processes, the message passing interfaces that enable real-time data exchange between localization, detection, control, and VR systems, the data recording and analysis frameworks that support system debugging and performance evaluation, and the launch file organization that enables easy switching between mapping and localization modes. The integration strategy will be examined, covering the synchronization mechanisms that ensure consistent timing across all system components, the error handling and recovery procedures that maintain system stability during component failures, and the calibration procedures required to achieve accurate sensor fusion and coordinate system alignment.

\section{Sensor Fusion Strategy}
This section will detail the conceptual approach developed for fusing UWB positioning data with RTABMap visual SLAM to achieve robust indoor localization. The sensor fusion rationale will be established, covering the complementary nature of UWB and visual SLAM technologies where UWB provides absolute positioning without drift while visual SLAM provides accurate orientation and short-term relative positioning, the challenges addressed by sensor fusion including UWB signal degradation in non-line-of-sight conditions and visual SLAM drift accumulation over extended operation, and the advantages gained through fusion including improved localization reliability and reduced dependence on environmental conditions. The fusion architecture will be presented, covering the decision to use UWB for global position estimation while relying on RTABMap for orientation data, the coordinate system transformations required to align UWB and camera coordinate frames, and the filtering and smoothing strategies employed to handle measurement noise and temporary signal loss. The implementation strategy will be detailed, covering the ROS2 message structures used to combine positioning data from multiple sources, the timing synchronization mechanisms that ensure consistent sensor fusion despite different update rates, and the parameter tuning approaches used to optimize fusion performance for the specific operating environment. The validation methodology will be outlined, covering the experimental protocols developed to assess fusion accuracy compared to individual sensor performance, the test scenarios designed to evaluate system behavior under challenging conditions like UWB signal obstruction or visual feature scarcity, and the metrics developed to quantify localization accuracy and reliability improvements achieved through sensor fusion.

\section{Atomic Movement Control Architecture}
This section will present the conceptual framework developed for the atomic movement control system that enables predictable VR interaction with the physical robot. The atomic movement concept will be established, covering the principle that all robot movements must be discrete, completion-guaranteed operations that provide clear feedback to VR users, the four-state control architecture that separates robot behaviors into rest, attention-getting, preparation, and execution phases, and the synchronization requirements that ensure coordinated behavior between base locomotion and leg articulation systems. The VR interaction design will be detailed, covering the mapping of VR user actions to appropriate robot movement commands, the feedback mechanisms that inform users about robot state and movement completion, and the safety considerations that prevent conflicting commands and ensure predictable robot behavior. The movement coordination strategy will be presented, covering the locking mechanisms that prevent simultaneous conflicting movements, the command queuing system that handles user inputs during ongoing movements, and the timing optimization that ensures natural-looking robot behavior while maintaining system responsiveness. The state machine architecture will be examined, covering the discrete states defined for leg and base controllers, the transition conditions that govern movement between states, and the error handling procedures that maintain system stability when movements cannot be completed as planned. The pulse command system will be detailed, covering the implementation of discrete command signals that automatically return to idle state, the timing parameters that ensure complete movement execution, and the integration with the VR system that provides appropriate user feedback for each movement phase.

\section{Real-time Performance Optimization}
This section will address the conceptual approaches developed to achieve real-time performance across all system components despite the computational demands of simultaneous SLAM, human detection, and VR communication. The performance requirements analysis will be established, covering the timing constraints imposed by VR interaction that require low-latency response to user inputs, the computational load distribution across SLAM processing, pose detection, and system coordination tasks, and the memory management strategies required for sustained operation on embedded hardware with limited resources. The optimization strategy will be presented, covering the parallel processing architecture that distributes computational load across multiple ROS2 nodes, the priority scheduling that ensures critical functions like localization maintain consistent performance, and the resource allocation strategies that balance accuracy with computational efficiency. The real-time processing pipeline will be detailed, covering the image processing optimization that enables simultaneous SLAM and human detection from the same camera stream, the communication optimization that minimizes latency in VR data exchange, and the memory management that prevents performance degradation during extended operation. The scalability considerations will be examined, covering the modular architecture that allows selective activation of system components based on current requirements, the configuration management that enables performance tuning for different operational scenarios, and the monitoring systems that provide real-time feedback about system performance and resource utilization to support ongoing optimization efforts.
