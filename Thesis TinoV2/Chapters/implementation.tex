This chapter details the comprehensive implementation of the Tino V2 robot system, covering the complete redesign and upgrade of the platform. The implementation encompasses the transition from legacy Raspberry Pi-based architecture to a modern ROS2-based system running on NVIDIA Orin Nano, the integration of advanced sensing capabilities including SLAM and human detection, hardware redesign for improved reliability and performance, and the development of VR integration capabilities. Each section provides detailed technical implementation details, design decisions, and validation results that demonstrate the enhanced capabilities of the Tino V2 platform.

\section{ROS2 Architecture Design and Implementation}

The migration from the legacy monolithic Python architecture to ROS2 represents a fundamental paradigm shift in Tino's system design. The Robot Operating System 2 framework provides the distributed computing foundation necessary to leverage the NVIDIA Orin Nano's enhanced computational capabilities while addressing the scalability and reliability limitations of the original Raspberry Pi implementation.

The ROS2 framework selection was driven by several critical technical advantages over the legacy system. The Data Distribution Service (DDS) middleware provides robust inter-process communication with Quality of Service (QoS) guarantees, enabling reliable data transmission even under high computational loads. The real-time scheduling capabilities ensure deterministic message delivery for time-critical operations such as motor control and sensor fusion. The modular architecture allows independent development and testing of subsystems, dramatically improving development efficiency and system maintainability.

The distributed processing capabilities of ROS2 enable optimal utilization of the Orin Nano's multi-core ARM Cortex-A78AE CPU and integrated GPU. Critical processes such as SLAM computation, human pose detection, and sensor fusion can execute in parallel without blocking the main control loop. The standardized message interfaces facilitate seamless integration of new sensors and capabilities, while the discovery mechanisms enable automatic node detection and connection during system startup.

\subsection{Node Structure and Functionality}

The Tino V2 system architecture consists of six primary ROS2 nodes, each responsible for specific subsystem functionality while maintaining loose coupling through standardized message interfaces.

\subsubsection{Gamepad Control Node}

The \texttt{gamepad\_node.py} implements Xbox controller input handling specifically for development and testing purposes. During actual experimental operation, this node is disabled as VR control messages completely replace gamepad input. The node addresses the D-input to X-input compatibility issues commonly encountered on Linux-based systems through proper driver configuration and input mapping.

The pulse generation mechanism replaces continuous joystick input with discrete 3-cycle command pulses that automatically return to idle state. Each button press triggers a complete command sequence lasting approximately 120ms (3 cycles at 25Hz), ensuring that movements produce predictable robot responses during testing. The node publishes commands to \texttt{base\_cmd\_vel} and \texttt{head\_cmd} topics using the same message format as the VR system.

The implementation includes comprehensive error handling for gamepad connectivity issues, deadzone management for analog inputs, and automatic device detection for Logitech F710 controllers. Button mapping follows the VR command structure with face buttons controlling leg states (X=state 1, Y=state 2, B=state 3, A=state 0) and bumpers triggering rotation commands combined with leg state 3.

\subsubsection{Hardware Interface Node}

The \texttt{hardware\_interface\_node.py} manages serial communication with three distinct Arduino subsystems through dedicated device symlinks: \texttt{/dev/ttyBASE}, \texttt{/dev/ttyLEG}, and \texttt{/dev/ttyHEAD}. The implementation addresses device identification challenges through udev rules that create consistent device paths based on Arduino serial numbers.

The node implements parallel serial communication threads for each Arduino subsystem, enabling simultaneous command transmission and status monitoring. Each thread operates at 115200 baud with configurable message repetition (default 3 repetitions) to ensure reliable command delivery. The command format follows the structure: \texttt{BF:value\_BB:value\_HP:value\_HX:value\_HY:value}, where BF represents base forward movement (leg states), BB represents base rotation, HP controls head pitch, and HX/HY control head pan and tilt respectively.

Error handling includes automatic device discovery, connection monitoring, and graceful degradation when individual Arduino systems become unavailable. The node publishes Arduino feedback messages to the \texttt{arduino\_feedback} topic and provides comprehensive debugging capabilities through configurable logging levels.

\subsubsection{Robot Controller Node}

The \texttt{robot\_controller\_node.py} serves as the central coordination hub managing all robot behaviors, localization monitoring, and sensor fusion operations. Beyond basic command forwarding, the node implements sophisticated localization system supervision including RTAB-Map orientation loss detection, UWB positioning integration, and automatic recovery procedures.

The localization monitoring system continuously analyzes incoming pose data from RTAB-Map to detect the specific orientation values (quaternion: x=1.0, y=0.0, z=0.0, w=0.0) that indicate odometry loss. Upon detection, the node automatically triggers the \texttt{/reset\_odom} service and activates orientation estimation based on movement direction calculated from position history. The system maintains a 5-position movement history to enable orientation estimation when RTAB-Map orientation becomes unreliable.

Sensor fusion capabilities combine UWB absolute positioning with RTAB-Map orientation data, applying a configurable 11.5-degree rotation correction to align coordinate frames. The node publishes fused pose data to \texttt{/vr\_in/robot\_pose} and forwards human detection information from \texttt{/human\_position} and skeleton data from \texttt{/human\_skeleton} topics to VR interface systems. Audio integration enables bidirectional communication between VR systems and robot microphone/speaker hardware through \texttt{/vr\_in/audio\_output} and \texttt{/vr\_out/audio\_input} topics.

Performance monitoring includes comprehensive logging of sensor fusion status, communication health tracking, and diagnostic reporting that enables rapid identification of localization or communication issues during operation.

\subsubsection{VR Interface Node}

The \texttt{vr\_interface\_node.py} handles all VR system integration through a custom UDP communication protocol that completely replaces any TCP-based communication systems. The node implements bidirectional data exchange with Unity applications using three dedicated UDP ports: port 5005 for incoming VR commands, port 5006 for outgoing robot pose data, and port 5007 for human skeleton transmission.

The incoming message processing handles 32-byte UDP packets containing VR control data: 3 floats for head control (pitch, pan, tilt), 2 integers for base commands (state 0-3, angular direction -1/0/1), 2 values for audio control (volume and orientation), and 1 integer for message ordering. The node implements sophisticated message ordering validation to detect lost or duplicate packets and automatic VR reconnection handling that resets message counters upon connection restoration.

Outgoing data transmission operates at configurable rates (default 10Hz for pose data, 10Hz for skeleton data) with separate UDP channels to prevent interference. The pose data packets contain 24 bytes with fused position and orientation information, while skeleton packets transmit exactly 17 COCO-format joints in 208-byte messages. The node maintains comprehensive communication health monitoring, including rate validation, connection status tracking, and detailed diagnostic logging for system maintenance.

\subsubsection{Pose Detection Node}

The \texttt{pose\_detection\_node.py} implements real-time human detection and skeleton tracking using YOLOv11 optimized with TensorRT for Orin Nano performance. The node subscribes to camera topics from the Oak-D Pro (\texttt{/right/image\_rect}, \texttt{/stereo/depth}, \texttt{/stereo/camera\_info}) and publishes detection results to multiple topics for different system components.

Detection processing combines 2D pose estimation with stereo depth information to generate 3D skeleton tracking. The node publishes human position data to \texttt{/human\_position}, skeleton visualization markers to \texttt{/human\_skeleton}, and structured pose arrays to \texttt{/human\_skeleton\_poses}. The implementation includes depth calibration with configurable scale factors and outlier rejection to ensure consistent 3D positioning across varying distances.

The node implements closest-person selection algorithms and temporal smoothing to reduce detection jitter while maintaining real-time performance. Comprehensive parameter configuration enables adjustment of confidence thresholds, depth processing parameters, and logging verbosity for different operational scenarios.

\subsection{Communication Protocols and Message Design}

The ROS2 communication infrastructure implements a sophisticated message protocol hierarchy designed for reliable and efficient data exchange between all system components. The architecture utilizes topic-based publish-subscribe messaging and service-based communication for different operational requirements.

\subsubsection{Topic-Based Messaging}

The primary communication mechanism utilizes topic-based publish-subscribe messaging that enables decoupled component interaction. Critical data streams include robot pose information, human detection results, sensor data, and control commands. Each topic implements appropriate QoS policies to ensure reliable delivery while optimizing for latency and bandwidth requirements.

Robot pose data on \texttt{/vr\_in/robot\_pose} utilizes reliable delivery policy with history depth of 10 messages to ensure VR systems receive consistent positioning information. Human skeleton data on \texttt{/human\_skeleton} and \texttt{/human\_skeleton\_poses} implements best-effort delivery policy optimized for real-time performance, as occasional message loss is acceptable for continuous tracking applications. Control commands on \texttt{base\_cmd\_vel} and \texttt{head\_cmd} utilize reliable delivery with immediate processing to ensure critical movement commands reach their destinations.

The topic hierarchy follows a logical structure reflecting data flow: input topics (\texttt{/vr\_out/cmd\_vel}, \texttt{/vr\_out/head\_cmd}, \texttt{/vr\_out/audio\_input}) carry commands from external systems, processing topics (\texttt{base\_cmd\_vel}, \texttt{head\_cmd}) handle internal robot control, and output topics (\texttt{/vr\_in/robot\_pose}, \texttt{/vr\_in/human\_position}, \texttt{/vr\_in/audio\_output}) provide data to external systems.

\subsubsection{Custom Message Definitions}

The system utilizes standard ROS2 message types with specific conventions for Tino's operational requirements. Robot pose information uses \texttt{geometry\_msgs/PoseStamped} messages containing 3D position and quaternion orientation with high-precision timestamps for sensor fusion algorithms. The messages include coordinate frame information (\texttt{oak\_right\_camera\_optical\_frame} for detection data) to support proper coordinate transformations.

Human detection utilizes \texttt{geometry\_msgs/PoseArray} for skeleton joint positions, containing exactly 17 COCO-format joints with consistent 3D coordinates even for missing or occluded body parts. Visualization data uses \texttt{visualization\_msgs/MarkerArray} for RViz display and debugging purposes. Audio communication employs \texttt{std\_msgs/Int16MultiArray} for raw PCM audio samples and \texttt{std\_msgs/Float32MultiArray} for processed audio parameters.

VR command messages utilize \texttt{geometry\_msgs/Twist} for movement commands where \texttt{linear.x} carries leg state values (0-3) and \texttt{angular.z} carries rotation commands (-1, 0, 1). Head commands use the same message type with \texttt{angular.x}, \texttt{angular.y}, and \texttt{angular.z} representing pitch, pan, and tilt respectively.

\subsubsection{Service Communication}

Synchronous operations requiring immediate responses utilize ROS2 service calls. The odometry reset functionality implements the \texttt{/reset\_odom} service using \texttt{std\_srvs/Empty} message type, enabling the robot controller to trigger RTAB-Map odometry reset when orientation loss is detected. The service call includes proper error handling and callback mechanisms to confirm successful execution.

System configuration changes and diagnostic queries implement service-based communication to ensure proper execution and immediate feedback. The architecture supports extensible service interfaces for future functionality such as map management, calibration procedures, and advanced diagnostic operations.

\subsection{Integration with External Systems}

The ROS2 architecture facilitates seamless integration with external systems through custom UDP protocols and standardized interfaces, significantly enhancing the research capabilities and operational flexibility of the Tino V2 platform.

\subsubsection{Monitoring and Debugging Infrastructure}

The ROS2 architecture enables sophisticated monitoring and debugging capabilities through comprehensive logging systems and real-time diagnostic tools. Each node implements configurable logging levels that can be adjusted dynamically without system restart, enabling detailed debugging during development while maintaining optimal performance during operation.

Communication health monitoring tracks message rates, connection status, and data flow statistics across all system components. The VR interface node implements rate validation that compares actual data rates against expected values, providing immediate notification of communication problems. The robot controller node monitors sensor fusion performance and provides detailed diagnostics for localization system health.

Remote debugging capabilities enable system monitoring and control through standard ROS2 tools including \texttt{ros2 node info}, \texttt{ros2 topic echo}, and custom diagnostic interfaces. The modular architecture supports selective node restart and configuration adjustment without affecting overall system operation.

\subsubsection{Extensibility and Configuration Management}

The modular architecture enables straightforward addition of new sensors and capabilities through standardized topic interfaces. New detection or sensing nodes integrate seamlessly by publishing to established topic hierarchies, while new control systems can subscribe to existing data streams without modification of core system components.

Launch file configuration provides flexible system deployment with separate configurations for development testing (with gamepad control), VR operation (with UDP communication), and research data collection. Parameter management enables environment-specific optimization including network addresses, communication rates, sensor calibration values, and performance tuning parameters.

The standardized interfaces support integration with external analysis tools and research systems. Data recording nodes can subscribe to any combination of system topics for comprehensive interaction analysis, while external control systems can inject commands through standard ROS2 interfaces. The architecture supports distributed deployment across multiple computing platforms and future integration with cloud-based services.

\section{SLAM and Sensor Fusion Implementation}

The localization system represents one of the most critical upgrades in Tino V2, implementing a sophisticated hybrid approach that combines visual SLAM capabilities with absolute positioning to achieve robust, accurate localization in dynamic social environments. The implementation addresses the fundamental limitations of pure visual odometry while leveraging the strengths of both RTABMap visual SLAM and Ultra-Wideband positioning technologies.

\subsection{RTABMap Integration with Oak-D Pro Camera}

The RTABMap implementation utilizes the Oak-D Pro stereo camera system to provide visual-inertial odometry and mapping capabilities. The integration leverages the DepthAI ecosystem through the \texttt{depthai\_examples} package, which provides optimized camera drivers and ROS2 integration for the OAK platform.

\subsubsection{Camera System Configuration}

The Oak-D Pro integration utilizes stereo vision with synchronized RGB and depth image streams. The camera configuration operates at 400p mono resolution to balance processing performance with image quality on the Orin Nano platform. The system publishes synchronized image streams on \texttt{/right/image\_rect}, depth data on \texttt{/stereo/depth}, and camera calibration information on \texttt{/right/camera\_info}.

The \texttt{stereo\_inertial\_node.launch.py} from the depthai package initializes the camera with depth alignment disabled (\texttt{depth\_aligned: false}) to maintain processing efficiency. The IMU data stream on \texttt{/imu} provides inertial measurements for visual-inertial odometry, while RViz visualization is disabled for headless operation (\texttt{enableRviz: false}).

Camera calibration utilizes the factory calibration data embedded in the Oak-D Pro hardware, ensuring accurate depth estimation and stereo baseline measurements. The integration maintains the \texttt{oak-d-base-frame} as the primary coordinate reference, enabling consistent coordinate transformations throughout the system.

\subsubsection{RTABMap Node Configuration}

The RTABMap implementation utilizes four specialized nodes that work in concert to provide robust SLAM functionality. The \texttt{rgbd\_sync} node from the \texttt{rtabmap\_sync} package ensures temporal alignment of RGB and depth image streams, critical for accurate feature matching and depth association.

The \texttt{rgbd\_odometry} node from \texttt{rtabmap\_odom} implements visual odometry using the synchronized image streams, providing continuous pose estimation even during mapping interruptions. The main \texttt{rtabmap} node from \texttt{rtabmap\_slam} handles loop closure detection, map management, and long-term localization capabilities.

IMU integration utilizes the \texttt{imu\_filter\_madgwick\_node} for quaternion computation from raw IMU data. The filter operates in ENU (East-North-Up) world frame without magnetic field compensation (\texttt{use\_mag: false}) and disables transform publishing (\texttt{publish\_tf: false}) to prevent conflicts with the main localization system.

\subsubsection{Database and Memory Management}

The RTABMap database storage utilizes the home directory location \texttt{\textasciitilde/rtabmap.db} for persistent map storage. The database contains visual features, loop closure information, and occupancy grid data that enable relocalization across different operational sessions. Memory management parameters optimize performance for continuous operation without degradation during extended mapping sessions.

The system implements appropriate buffer management and feature detection rates optimized for the social robotics application domain. The configuration balances memory usage against map quality to ensure sustainable long-term operation while maintaining sufficient detail for reliable relocalization.

\subsection{SLAM Mapping and Localization Modes}

The system implements distinct operational modes that enable both map creation and localization-only operation, providing flexibility for different deployment scenarios and research requirements.

\subsubsection{Mapping Mode Implementation}

The mapping mode utilizes the \texttt{rtab\_mapping.launch.py} configuration that enables full SLAM functionality including map building, loop closure detection, and visual feature database creation. The launch file initializes the complete RTABMap pipeline with mapping-optimized parameters.

Key mapping parameters include \texttt{subscribe\_rgbd: True} for synchronized color and depth processing, \texttt{subscribe\_odom\_info: True} for enhanced odometry integration, and \texttt{approx\_sync: False} for precise temporal alignment. The \texttt{wait\_imu\_to\_init: True} parameter ensures proper IMU initialization before beginning mapping operations.

The mapping process operates with continuous loop closure detection and feature database updates. The system maintains visual landmarks and occupancy grid information that supports both immediate navigation and future relocalization. Map visualization through \texttt{rtabmap\_viz} enables real-time monitoring of mapping progress and quality assessment.

\subsubsection{Localization Mode Implementation}

The localization mode utilizes the \texttt{rtab\_localization.launch.py} configuration that loads existing maps and disables new map creation. Critical parameters include \texttt{localization: True} to enable localization-only mode and \texttt{Mem/IncrementalMemory: False} to disable memory updates that would modify the existing map.

The \texttt{Rtabmap/DetectionRate: 3.0} parameter optimizes loop closure detection frequency for localization scenarios, balancing computational load against relocalization performance. The system loads the existing database and continuously tracks robot position within the known map structure.

Relocalization capabilities enable the robot to determine its position within previously created maps even after system restart or temporary tracking loss. The mode supports operation in known environments without requiring new map creation, essential for consistent experimental conditions.

\subsubsection{Mode Switching and Database Management}

The dual-mode architecture enables seamless transitions between mapping and localization operations through launch file selection. Map persistence utilizes the RTABMap database format that maintains visual features, loop closures, and occupancy grids across operational sessions.

Database management includes map saving and loading protocols that ensure data integrity during mode transitions. The system supports multiple map databases for different operational environments, enabling deployment across various research locations without map conflicts.

\subsection{Initial SLAM-Only System Limitations and Drift Issues}

Initial testing of the pure RTABMap implementation revealed significant limitations that necessitated the development of the hybrid sensor fusion approach. Comprehensive testing documented systematic failures that compromised localization accuracy during extended operation.

\subsubsection{Drift Accumulation Analysis}

Extended operation testing revealed position drift accumulation reaching up to 1.2 meters during 30-minute operational sessions. The drift manifested as gradual position error accumulation that increased monotonically with operation time, particularly during movements in feature-poor environments or areas with repetitive visual patterns.

Error source analysis identified visual odometry drift as the primary contributor, exacerbated by lighting changes, motion blur during movement, and insufficient visual features near walls and uniform surfaces. The accumulation proved particularly problematic for VR applications requiring precise robot positioning for immersive interaction.

Statistical analysis of position errors demonstrated systematic bias in specific movement directions, indicating calibration issues and environmental factors affecting feature detection consistency. The four-position testing protocol revealed repeatability issues with standard deviations exceeding 40cm for identical positioning commands.

\subsubsection{Relocalization Challenges}

RTABMap relocalization failures occurred frequently in environments with insufficient distinctive features, requiring manual intervention through robot rotation to achieve sufficient visual feature matching. Feature-poor environments near walls or in corners consistently caused tracking loss, necessitating operator intervention to restore localization.

Map corruption events required complete database reconstruction when loop closure detection failed catastrophically. These failures typically occurred during rapid movement or in areas with dynamic lighting conditions, compromising the entire mapping session and requiring restart from known positions.

The relocalization process proved unreliable for autonomous operation, as successful position recovery often required specific robot orientations and environmental conditions that could not be guaranteed during normal social interaction scenarios.

\subsection{UWB Positioning System Implementation}

The Ultra-Wideband positioning system provides absolute position reference to complement visual SLAM, addressing the drift and relocalization limitations identified in pure SLAM operation. The UWB integration utilizes a third-party positioning package that interfaces with DecaWave hardware through serial communication.

\subsubsection{Hardware Configuration and Integration}

The UWB system utilizes the \texttt{uwb\_positioning} package configured through the \texttt{uwb.launch.py} file. The launch configuration specifies serial communication parameters including \texttt{serial\_port\_name: /dev/ttyACM0} and \texttt{serial\_baud\_rate: 115200} for interface with the UWB hardware module.

The UWB tag integrates directly with the Tino robot platform with minimal interference to other systems. Anchor placement follows a strategic configuration that ensures optimal coverage of the operational environment while minimizing Non-Line-of-Sight (NLOS) conditions that degrade positioning accuracy.

The system publishes absolute position data on the \texttt{/UWB/Pos} topic using \texttt{geometry\_msgs/Pose} message format, providing 3D coordinates that serve as the absolute reference for sensor fusion algorithms.

\subsubsection{Positioning Algorithm and Performance}

The UWB system implements multilateration techniques that calculate 3D position from time-of-flight measurements to multiple anchor points. The positioning algorithm includes NLOS mitigation strategies and noise filtering to provide stable position estimates under typical indoor operational conditions.

Real-time performance characteristics include update rates suitable for robot control applications with latency measurements demonstrating compatibility with real-time system requirements. Accuracy evaluation shows centimeter-level precision under optimal conditions, with graceful degradation in challenging RF environments.

\subsection{Sensor Fusion Between RTABMap Orientation and UWB Positioning}

The hybrid localization system implements sophisticated sensor fusion that combines the complementary strengths of visual SLAM and UWB positioning. The fusion approach separates position and orientation estimation, utilizing UWB for absolute position reference while maintaining RTABMap for orientation data.

\subsubsection{Fusion Algorithm Implementation}

The sensor fusion implementation in the robot controller node utilizes a practical approach that combines UWB absolute positioning with RTABMap orientation data. The \texttt{\_create\_fused\_pose} method implements the core fusion logic that creates unified pose estimates from multiple sensor inputs.

The fusion algorithm prioritizes UWB position data when available, falling back to RTABMap position estimates only when UWB communication fails. Position data from UWB undergoes coordinate frame transformation to align with the RTABMap reference frame, utilizing a configurable 11.5-degree rotation correction applied through the \texttt{\_apply\_rotation\_to\_pose} method.

Orientation estimation maintains RTABMap quaternion data as the primary source, implementing sophisticated validation to detect orientation loss conditions. The system monitors for specific quaternion values (x=1.0, y=0.0, z=0.0, w=0.0) that indicate RTABMap odometry failure and automatically triggers recovery procedures.

\subsubsection{Orientation Loss Detection and Recovery}

The implementation includes robust orientation loss detection that monitors RTABMap output for invalid quaternion values indicating system failure. Upon detection of orientation loss, the system automatically triggers the \texttt{/reset\_odom} service to restart RTABMap odometry while maintaining position tracking through UWB data.

Fallback orientation estimation utilizes movement direction analysis when RTABMap orientation becomes unreliable. The \texttt{\_estimate\_orientation\_from\_movement} method calculates orientation from position history, maintaining operational capability during RTABMap recovery periods. The system stores the last 5 position measurements to enable orientation estimation with minimum 5cm movement thresholds.

Recovery validation continuously monitors RTABMap orientation data to detect system recovery. Upon detection of valid orientation values, the system automatically transitions back to RTABMap orientation while maintaining UWB position data, ensuring seamless operation without manual intervention.

\subsubsection{Coordinate System Alignment}

The sensor fusion system implements coordinate transformation procedures that align UWB coordinates with the SLAM coordinate frame. The transformation includes rotational alignment through quaternion multiplication that corrects for installation and calibration differences between sensor coordinate systems.

The robot controller implements dynamic coordinate frame management that handles different reference frames from various sensors. Position data undergoes proper transformation from \texttt{oak\_right\_camera\_optical\_frame} for camera-based detections while maintaining consistency with UWB absolute coordinates.

Calibration protocols enable adjustment of the coordinate alignment parameters without system modification. The 11.5-degree rotation correction represents empirically determined alignment that ensures consistent positioning across different operational scenarios and environmental conditions.

\subsubsection{Performance Monitoring and Diagnostics}

The fusion system implements comprehensive performance monitoring that tracks sensor health, fusion quality, and system reliability. The robot controller node maintains detailed statistics on UWB availability, RTABMap orientation validity, and fusion algorithm performance.

Communication health monitoring tracks message rates from both UWB and RTABMap systems, providing immediate notification of sensor failures or communication problems. The system logs fusion status information including orientation source selection (RTABMap, movement estimation, or fallback) and position source preference (UWB or RTABMap).

Diagnostic capabilities include real-time logging of sensor fusion decisions, position accuracy validation through movement consistency checking, and comprehensive error reporting that enables rapid identification of localization issues during operation. The monitoring system provides configurable logging levels to balance debugging information with system performance.


\section{Kinematic Base Upgrade from Omnidirectional to Differential Drive}
This section will detail the comprehensive redesign of Tino's mobility system, transitioning from the problematic omnidirectional Triksta base to a robust differential drive architecture. The limitations of the original omnidirectional system will be analyzed first, covering the mechanical failures experienced with the omniwheel rollers that became squared due to Tino's 20kg weight, the dragging issues with the rear wheel that occurred during forward and turning movements, and the unreliable motor performance under the sustained loads required for social robot operation. The differential drive design rationale will be explained, including the simplified kinematics that eliminate the complexity of omnidirectional control while maintaining adequate maneuverability for social interaction scenarios, the improved weight distribution that reduces stress on individual components, and the enhanced reliability achieved through proven mechanical design principles. The mechanical implementation will be detailed, covering the construction of the T-structure using aluminum Item profiles that provide a dynamic and adjustable framework, the motor mounting system modifications required to accommodate the new differential drive configuration, and the wheel positioning optimization that achieves proper balance and traction for the robot's operational requirements. The control system adaptation will be examined, including the implementation of custom PID (Proportional-Integral-Derivative) controllers specifically designed for differential drive kinematics, the motor driver upgrade to the more powerful MDD10A units that can handle increased loads, and the command interface modifications that maintain compatibility with existing movement control systems while improving performance and reliability.

\section{Power Supply System Redesign for Orin Nano}
This section will present the comprehensive power system redesign required to support the NVIDIA Orin Nano platform and associated high-performance components. The power requirements analysis will be detailed first, covering the Orin Nano's 19V DC input requirement and power consumption characteristics that reach up to 2A during maximum computational load, the additional power needs for the Oak-D Pro camera and onboard router systems, and the total system power budget that necessitated complete redesign of the legacy Raspberry Pi power architecture. The DC-DC converter implementation will be explained, including the selection and testing of the Oumefar 12V to 19V step-up converter that provides stable power delivery, the power efficiency analysis that demonstrates optimal battery utilization, and the thermal management considerations that ensure reliable operation under sustained loads. The battery system optimization will be examined, covering the consolidation from four separate battery systems to three integrated power sources, the 5200mAh 80C 11.1V 57.72Wh battery specification that provides approximately 1.37 hours of operation at maximum load, and the realistic operational time estimates of 2-3 hours under typical social interaction scenarios. The cable harness redesign will be detailed, including the removal of legacy USB-A and USB-C connections that were used for Raspberry Pi power delivery, the implementation of proper 12V input distribution and 19V DC jack connectivity, and the integration of the 12V to 5V converter that powers the onboard router and camera systems independently, providing flexibility for future system expansions and reducing the computational load on the main platform.

\section{Stewart Platform Head Mechanism Improvements}
This section will document the iterative design improvements made to Tino's Stewart platform head mechanism to address reliability issues and enhance performance under operational loads. The original system limitations will be analyzed first, covering the servo axis misalignment problems that created excessive stress on servo motors during head movements, the structural flex issues in the connecting arms that caused mechanical instability and reduced precision, and the repeated arm failures that occurred due to inadequate load distribution and material selection. The first design iteration will be detailed, including the servo axis alignment improvement that redirected forces through the head structure rather than the servo mechanisms, the 3D printed PLA arm replacement with enhanced geometry for improved load distribution, and the initial performance evaluation that showed reduced servo stress but continued structural flex issues. The final design implementation will be examined, covering the adoption of rod end (heim joints) on both ends of each Stewart platform arm to eliminate binding and allow free rotation, the combination of 3D printed components with metal heim joints that provides optimal balance between cost and performance, and the mechanical trade-offs including acceptable head wobble during stationary periods that may actually enhance the robot's expressive capabilities. The performance validation will be discussed, including load testing that demonstrates improved reliability under operational conditions, the movement precision evaluation that shows maintained accuracy despite the mechanical improvements, and the longevity testing that validates the enhanced design's suitability for extended social interaction scenarios.

\section{Camera Integration and Mounting Solutions}
This section will detail the comprehensive camera integration system developed to address the unique challenges of mounting sophisticated sensing equipment within Tino's soft fabric structure. The mounting system design will be explained first, covering the tripod-based camera support system that provides stable mounting for the Oak-D Pro camera, the bracket design that ensures proper camera alignment and minimizes vibration during robot movement, and the integration with the existing Stewart platform head that allows synchronized camera and head movements. The fabric integration challenges will be analyzed, including the camera visibility requirements that necessitate fabric modification without compromising Tino's aesthetic appeal, the heat dissipation needs of the Oak-D Pro camera that require ventilation considerations, and the protection requirements that shield sensitive camera components from physical damage during social interactions. The camera shell development will be detailed, covering the custom enclosure design that provides protection while maintaining cooling airflow, the velcro attachment system that secures fabric positioning without interfering with camera operation, and the mesh covering implementation that conceals the camera from casual observation while maintaining full operational capability. The field of view optimization will be examined, including the fabric positioning strategies that prevent interference with camera sensing, the testing procedures that validate optimal camera performance under various fabric configurations, and the reliability evaluation that ensures consistent operation throughout extended social interaction sessions.

\section{Audio System Integration}
This section will present the comprehensive audio system implementation that enables bidirectional communication capabilities for VR integration and enhanced human-robot interaction. The hardware component selection will be detailed first, covering the iTalk-01 omnidirectional microphone specification and mounting considerations within the fabric head structure, the speaker system selection and placement optimization that provides clear audio output without interfering with other robot systems, and the audio processing requirements that enable real-time communication with VR systems. The integration challenges will be analyzed, including the acoustic isolation needed to prevent feedback between microphone and speakers, the cable routing through the robot's structure that maintains mechanical flexibility while ensuring reliable connections, and the power management considerations that integrate audio components with the overall system power budget. The software implementation will be examined, covering the audio\_node.py and audio\_loopback.py ROS2 nodes that handle audio capture and playbook, the bidirectional communication protocols that enable seamless VR audio integration, and the audio processing algorithms that ensure high-quality sound transmission and reception. The performance validation will be discussed, including audio quality testing that demonstrates suitable performance for human-robot communication, latency measurements that verify real-time communication capabilities, and integration testing that validates seamless operation with the VR system and overall robot behavior control.

\section{YOLOv11 Pose Detection Implementation with TensorRT Optimization}
This section will detail the implementation of YOLOv11 pose detection system optimized for real-time performance on the NVIDIA Orin Nano platform using pre-trained models. The YOLOv11 architecture selection will be explained first, covering the advantages of using the latest YOLO iteration for pose estimation tasks, including improved accuracy in detecting multiple humans simultaneously and optimized network architecture that balances detection accuracy with computational efficiency for embedded platforms. The pre-trained model utilization will be detailed, including the selection of the yolo11n-pose.pt model that provides an optimal balance between accuracy and computational requirements, the model format conversion from PyTorch (.pt) to ONNX (.onnx) format for cross-platform compatibility, and the final optimization to TensorRT engine (.engine) format that maximizes inference performance on the Orin Nano's GPU. The TensorRT optimization process will be examined, covering the engine generation procedures that optimize the neural network for the specific hardware platform and the memory allocation strategies that ensure efficient GPU utilization during real-time operation. The implementation architecture will be discussed, including the ROS2 node design that subscribes to camera topics from the Oak-D Pro and publishes human detection results, and the message publishing system that provides skeleton joint information with confidence scores for other system components.

\section{Stereo Depth Integration for 3D Human Positioning}
This section will present the integration of stereo depth information with 2D pose detection to achieve 3D human positioning capabilities using the Oak-D Pro camera system. The depth data utilization will be detailed first, covering how the stereo camera system provides depth information at detected keypoint locations, the depth value extraction process that determines 3D coordinates for each detected joint, and the coordinate system transformation from camera frame to robot coordinate frame. The 3D positioning methodology will be explained, including the process of combining 2D joint detections with corresponding depth values to create 3D skeleton representations and the real-time processing requirements that maintain system responsiveness while providing human positioning information. The practical implementation will be discussed, including how depth measurement works at varying distances and the integration with the robot's localization system to provide human positions relative to the robot's coordinate frame.

\section{Real-time Skeleton Tracking with 17 Key Body Joints}
This section will detail the skeleton tracking implementation that extracts and processes 17 standard COCO keypoints for human posture detection. The keypoint detection framework will be explained first, covering the 17 keypoints detected by YOLOv11 including nose, eyes, ears, shoulders, elbows, wrists, hips, knees, and ankles, and the confidence scoring system that indicates detection reliability for each joint. The data processing pipeline will be detailed, including the extraction of keypoint coordinates and confidence scores from YOLOv11 output, the organization of joint information into structured skeleton representations, and the real-time publishing of skeleton data through ROS2 topics. The message format and data flow will be discussed, including the custom ROS2 message structures that transmit skeleton data with timestamps and the integration with other system components that can utilize human pose information for robot operation and VR data recording.

\section{Performance Optimization and Real-time Operation}
This section will present the optimization strategies implemented to achieve real-time performance of the human detection system on the Orin Nano platform. The computational optimization techniques will be detailed first, covering the TensorRT engine utilization that maximizes GPU inference performance and the memory management strategies that prevent resource exhaustion during continuous operation. The real-time performance characteristics will be explained, including the frame rate capabilities achieved by the optimized system and the processing latency from camera input to pose detection output. The system integration optimization will be examined, covering how the pose detection system operates alongside other robot functions including SLAM, localization, and movement control without creating performance bottlenecks. The practical performance evaluation will be discussed, including testing under various operational scenarios and system stability during extended operation periods.

\section{Integration with System Architecture}
This section will detail the integration of human detection with Tino's overall system architecture and coordination with other robot components. The data flow architecture will be explained first, covering how human pose detection results are published through ROS2 topics and made available to other system components including robot controller nodes and navigation systems. The system coordination will be discussed, including how pose detection operates in parallel with other robot functions such as localization, movement control, and audio processing without creating performance bottlenecks. The ROS2 integration implementation will be examined, covering the message publishing system that provides skeleton joint information with timestamps and the node architecture that ensures reliable data transmission to other system components. Finally, the practical benefits will be addressed, covering how real-time human detection enhances the robot's operational awareness and enables improved human-robot interaction capabilities through better understanding of human presence and positioning.

\section{VR System Architecture and Unity Communication}
This section will detail the comprehensive VR integration system that enables remote control and monitoring of Tino through Unity-based VR environments. The VR interface architecture will be explained first, covering the ROS2 \texttt{vr\_interface\_node} that serves as the central communication bridge between the robot's ROS2 system and external Unity applications, and the UDP communication protocol that provides real-time bidirectional data exchange for low-latency VR interaction. The Unity integration capabilities will be detailed, including the message structures for sending robot control commands from VR to ROS2 topics, the data reception system that provides robot pose, human detection, and audio information to Unity for visualization and interaction, and the networking configuration that enables flexible deployment across different network environments. The communication monitoring system will be examined, covering the configurable send rates for pose and skeleton data transmission, the health monitoring that tracks communication status and detects connection failures, and the message ordering system that ensures reliable data delivery and duplicate detection. The VR data recording functionality will be discussed, including the comprehensive recording system that captures all VR-relevant data streams for offline analysis.

\section{Atomic Movement System Design and 4-State Control Architecture}
This section will present the revolutionary atomic movement system designed specifically for natural VR interaction, replacing the previous continuous control scheme with discrete, completion-guaranteed movements. The 4-state control framework will be explained first, covering the unified state architecture applied to both leg and base controllers where state 0 represents idle/resting position, state 1 implements expressive ``little push'' movements for attention-getting behaviors, state 2 provides timing synchronization cycles, and state 3 executes atomic movements that must complete before new commands can be processed. The leg controller implementation will be detailed, including the state 1 optimized 3-phase movement (50\% forward extension, 5\% pause, 45\% return), the state 2 forward extension to maximum reach with position locking mechanisms, and the state 3 return-to-neutral movement with button-press completion detection. The base controller design will be examined, covering the state 1 rapid forward-backward sequence for expressive pointing behaviors, the state 2 timing cycle that provides 1.5-second synchronization delay, and the state 3 atomic movements including forward translation and left/right rotation operations, each with 1.7-second execution duration. The synchronization architecture will be discussed, including the sophisticated locking system that prevents base state 3 execution until leg state 2 completion, and the pending command system that stores VR commands during ongoing operations and automatically executes them upon completion.

\section{Pulse-Based Command System for VR Integration}
This section will detail the pulse-based command architecture that ensures perfect correspondence between VR user actions and physical robot movements. The pulse generation system will be explained first, covering the replacement of continuous signal transmission with discrete 3-cycle command pulses that automatically return to idle state, ensuring each VR interaction triggers exactly one complete robot movement cycle. The gamepad integration modifications will be detailed, including the removal of analog joystick control in favor of discrete button-based state commands, and the implementation of pulse timing that provides consistent command duration regardless of user input duration. The VR command processing will be examined, covering the UDP packet structure that transmits head control data (pitch, pan, tilt), base movement commands (state and angular direction), and audio parameters (volume and orientation), all synchronized with message ordering for reliable delivery. The atomic guarantee system will be discussed, including the movement completion assurance that prevents partial operations, the state machine locks that maintain movement integrity, and the natural interaction flow that ensures VR users always observe complete robot actions rather than interrupted movements. The timing optimization will be addressed, covering the precise 1.5-second state 2 timing cycle, the 1.7-second state 3 movement duration, and the synchronization mechanisms that coordinate multi-component movements for realistic dragging simulation.

\section{Unity-ROS2 Communication Protocol and Message Structures}
This section will present the comprehensive communication protocol designed for robust Unity-VR to ROS2 integration with optimal performance and reliability. The UDP communication architecture will be explained first, covering the multi-port configuration with port 5005 for incoming VR commands, port 5006 for outgoing robot pose data, and port 5007 for human skeleton transmission, enabling parallel data streams without interference. The incoming message format will be detailed, including the 32-byte VR command packets containing 3 floats for head control (pitch, pan, tilt), 2 integers for base commands (state 0--3, angular direction $-1/0/1$), 2 values for audio control (volume and orientation), and 1 integer for message ordering to detect lost or duplicate packets. The outgoing data structures will be examined, covering the 24-byte robot pose packets with position and orientation data fused from UWB and RTAB-Map systems, and the 208-byte skeleton packets containing exactly 17 COCO-format joints with consistent 3D coordinates for missing or occluded body parts. The configurable transmission rates will be discussed, including independent control of pose data frequency (default 10Hz), skeleton data frequency (default 10Hz), and expected incoming command rate (default 25Hz) to optimize performance for different network conditions and VR application requirements. The monitoring and debugging capabilities will be addressed, covering the comprehensive logging system that tracks communication health, the rate validation that ensures expected data flow, and the error detection mechanisms that identify connection problems and provide detailed diagnostic information for system maintenance.

\section{Bidirectional Audio Communication and Spatial Processing}
This section will detail the advanced audio communication system that enables natural voice interaction between VR users and the physical robot environment. The audio data flow architecture will be explained first, covering the microphone input processing that captures robot-side audio and transmits it to VR systems through ROS2 topics, the VR audio reception that provides spatial audio information with volume and orientation parameters for immersive sound positioning, and the bidirectional communication that enables real-time voice interaction between VR users and people in the robot's physical environment. The audio processing implementation will be detailed, including the 16-bit PCM audio sample handling through Int16MultiArray message structures, the real-time audio streaming that maintains low latency for natural conversation flow, and the volume and orientation control system that allows VR applications to adjust audio characteristics based on virtual positioning and interaction context. The spatial audio integration will be examined, covering the orientation parameter system that provides directional audio information in degrees, the volume control mechanisms that enable distance-based audio attenuation simulation, and the Unity integration capabilities that support immersive audio experiences in VR environments. The practical applications will be discussed, including the human-robot interaction enhancement through voice communication, the remote presence capabilities that allow VR users to participate in physical environment conversations, and the research data collection features that record audio interactions for analysis of human-robot communication patterns and social interaction behaviors.
